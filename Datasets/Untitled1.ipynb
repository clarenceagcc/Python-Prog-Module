{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe033b-e590-4987-8b3c-484a92fa5844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c29612-80eb-46f1-a48c-bd42352184aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from selenium import webdriver\n",
    "import sys\n",
    "sys.path.insert(0,'/usr/bin/chromedriver')\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException , ElementClickInterceptedException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import lxml\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a73cd7-1844-41d2-8ee5-9955618e2505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckUnique(links):\n",
    "\n",
    "    \n",
    "        count_dict = {link:links.count(link) for link in links if links.count(link) > 1}\n",
    "        clean_links = [link for link in links if link not in list(count_dict.keys())]\n",
    "        return clean_links\n",
    "\n",
    "def ScrapeLinks(driver , start_pg , end_pg ,total_pages):\n",
    "\n",
    "        #Total pages are the total web pages on the website not the total you want to scrape\n",
    "        if end_pg > total_pages:\n",
    "            print(\"Enter valid Page Indexes\")\n",
    "        br = driver\n",
    "        scr_links = []\n",
    "        url_head = 'https://www.glassdoor.co.in'\n",
    "        #br = self.Initialize_Driver()\n",
    "        for pg_num in range(start_pg, end_pg+1):\n",
    "            url = 'https://www.glassdoor.co.in/Job/new-york-data-scientist-jobs-SRCH_IL.0,8_IC1132348_KO9,23_IP'+str(i)+'.htm?fromAge=14'\n",
    "            br.get(url)\n",
    "            br.implicitly_wait(5)\n",
    "\n",
    "            soup = BeautifulSoup(br.page_source , 'lxml')\n",
    "\n",
    "            #print(f\"Scraping Page Number : {pg_num}\")\n",
    "            for a_tag in soup.find_all('a' , href = True):\n",
    "                pattern = re.compile('/partner/jobListing')\n",
    "                if pattern.match(str(a_tag['href'])):\n",
    "                    scr_links.append(url_head + str(a_tag['href']))\n",
    "        \n",
    "        op_links = CheckUnique(scr_links)\n",
    "        if len(op_links) == ((end_pg-start_pg) +1)*30 :\n",
    "            print(\"Scraping of links is completed\")\n",
    "            return op_links\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "def ScrapeData(num_jobs,driver , links):\n",
    "\n",
    "        br = driver\n",
    "        #data_df = df\n",
    "        #cleaned_links = ScrapeLinks(br)\n",
    "        #if data_df is None:\n",
    "        data_df = pd.DataFrame(columns = ['Job_title' , 'Company' , 'State' , 'City' , 'Min_Salary' ,'Max_Salary' ,'Job_Desc' , \n",
    "                                         'Industry'  , 'Rating' , 'Date_Posted' , 'Valid_until' , 'Job_Type' ,\n",
    "                                          ])\n",
    "        \n",
    "        start = time.time()\n",
    "        print(\"Gathering Information\")\n",
    "        for job in range(num_jobs):\n",
    "\n",
    "            #print(f'Extracting information about job : {job}')\n",
    "            br.get(links[job])\n",
    "            br.implicitly_wait(10)\n",
    "\n",
    "            soup = BeautifulSoup(br.page_source , 'lxml')\n",
    "\n",
    "            try:\n",
    "                \n",
    "                #The rest of the data is available in scrip = 'application/ld+json' i.e in  json file\n",
    "                json_file = soup.find('script' , {'type':'application/ld+json'})\n",
    "                op_json = json.loads(str(json_file.text) , strict = False)\n",
    "                \n",
    "                #title\n",
    "                try:\n",
    "                    title = op_json['title']\n",
    "                    #print(title)\n",
    "                except:\n",
    "                    title  = None\n",
    "                \n",
    "                #Dateposted\n",
    "                try:\n",
    "                    post_date = op_json['datePosted']\n",
    "                except:\n",
    "                    post_date = None\n",
    "                \n",
    "                #type\n",
    "                try:\n",
    "                    job_type = op_json['employmentType']\n",
    "                except:\n",
    "                    job_type = None\n",
    "\n",
    "                #Annual Salary Range\n",
    "                try:\n",
    "                    min_salary = int(op_json['estimatedSalary']['value']['minValue'])\n",
    "                    max_salary = int(op_json['estimatedSalary']['value']['maxValue'])\n",
    "                except:\n",
    "                    min_salary = -1\n",
    "                    max_salary = -1\n",
    "                \n",
    "                #Job Posting validity date(Y-M-D)\n",
    "                try:\n",
    "                    valid_date = op_json['validThrough']\n",
    "                except:\n",
    "                    valid_date = None\n",
    "                \n",
    "                #Industry\n",
    "                try:\n",
    "                    industry = op_json['industry']\n",
    "                except:\n",
    "                    industry = None\n",
    "                \n",
    "                #Location\n",
    "                try:\n",
    "                    city = op_json['jobLocation']['address']['addressLocality']\n",
    "                    state = op_json['jobLocation']['address']['addressRegion']\n",
    "                except:\n",
    "                    city , state = None , None\n",
    "\n",
    "                #Company\n",
    "                try:\n",
    "                    company = op_json['hiringOrganization']['name']\n",
    "                except:\n",
    "                    company = None\n",
    "                \n",
    "                #Let's get Description\n",
    "                try:\n",
    "                    desc = soup.find(class_ = 'desc').text\n",
    "                except:\n",
    "                    desc = None\n",
    "                try:\n",
    "                    rating = soup.find('span' , {'class' : 'css-1pmc6te e11nt52q3'}).text.replace('â˜…' , '')\n",
    "                \n",
    "                except:\n",
    "                    rating = None\n",
    "\n",
    "\n",
    "            \n",
    "                data_df = data_df.append({'Job_title' : title,\n",
    "                                      'Company' : company,\n",
    "                                      'State' : state,\n",
    "                                      'City' : city,\n",
    "                                      'Min_Salary':min_salary,\n",
    "                                      'Max_Salary':max_salary,\n",
    "                                      'Job_Desc' : desc,\n",
    "                                      'Industry':industry,\n",
    "                                      'Rating':rating,\n",
    "                                      'Date_Posted' : post_date,\n",
    "                                      'Valid_until' : valid_date,\n",
    "                                      'Job_Type' :job_type} , ignore_index = True)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        #driver.close()\n",
    "        print(f\"Scraping Completed for {data_df.shape[0]} jobs\")\n",
    "        print(f\"Time Required : {time.time() - start} seconds\")\n",
    "        \n",
    "        return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a707a89-3ee9-4465-b276-6559e37803dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--disable-dev-shm-usage\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     13\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mservice, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[0;32m---> 15\u001b[0m links \u001b[38;5;241m=\u001b[39m \u001b[43mScrapeLinks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstart_pg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mend_pg\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m, in \u001b[0;36mScrapeLinks\u001b[0;34m(driver, start_pg, end_pg, total_pages)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#br = self.Initialize_Driver()\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pg_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_pg, end_pg\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.glassdoor.co.in/Job/new-york-data-scientist-jobs-SRCH_IL.0,8_IC1132348_KO9,23_IP\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[43mi\u001b[49m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.htm?fromAge=14\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m     br\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     20\u001b[0m     br\u001b[38;5;241m.\u001b[39mimplicitly_wait(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "#Please Specify number of jobs as an excessive number of requests made by the computer Ip can potentially block you\n",
    "#glassdoor has 30 listing per page, so you can calculate manually how much is the total amount of jobs present i.e (num_pages*30)\n",
    "num_jobs = 150\n",
    "start_pg =1 \n",
    "end_pg = 30\n",
    "total_pages = 30\n",
    "\n",
    "service = Service(executable_path='/usr/bin/chromedriver')\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage') \n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "links = ScrapeLinks(driver,start_pg,end_pg , 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6cc65-fe66-4d9f-b2e0-e1be06c1c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "links[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9006bc-b552-4fe1-b1d4-91a9daeb35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_jobs = 30*(end_pg - start_pg+1)\n",
    "f_df = pd.DataFrame(columns = ['Job_title' , 'Company' , 'State' , 'City' , 'Min_Salary' ,'Max_Salary' ,'Job_Desc' , \n",
    "                                         'Industry'  , 'Rating' , 'Date_Posted' , 'Valid_until' , 'Job_Type' ,\n",
    "                                          ])\n",
    "dummy_df  = ScrapeData(10 , driver , links)\n",
    "#Exctract 150 jobs every 1 mins \n",
    "#for i in range(total_jobs//num_jobs):\n",
    "     #df = ScrapeData(num_jobs , driver , links[num_jobs*i : num_jobs*(i+1)])\n",
    "     #f_df = f_df.append(df)\n",
    "     #time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c08eb5-f258-4e61-a800-ddea027d54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the file to csv\n",
    "f_df.to_csv('Data_Job_SG' , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf7758-5fbd-408e-8c37-7b92093b89e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
